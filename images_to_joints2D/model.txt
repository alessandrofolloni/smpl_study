Input Layer (size: input_size)
       |
       v
[Hidden Layer 1]
   Linear (input: input_size -> output: h1)
       |
       v
   ReLU Activation
       |
       v
   Dropout (rate: dropout)
       |
       v
[Hidden Layer 2]
   Linear (input: h1 -> output: h2)
       |
       v
   ReLU Activation
       |
       v
   Dropout (rate: dropout)
       |
       v
...
       |
       v
[Hidden Layer n]
   Linear (input: h(n-1) -> output: hn)
       |
       v
   ReLU Activation
       |
       v
   Dropout (rate: dropout)
       |
       v
Output Layer
   Linear (input: hn -> output: output_size)